import pandas as pd
import glob
import os
import sys

# --- é…ç½®å‚æ•° ---
# 1. append.py çš„è¾“å‡ºæ–‡ä»¶å (ä¸Šä¸‹åˆå¹¶ç»“æœ)
APPEND_OUTPUT_FILENAME = 'cross.csv'

# 2. concat.py çš„è¾“å…¥æ–‡ä»¶ (ä¸»ä¿¡æ¯æ–‡ä»¶)
CONCAT_INPUT_DF1 = 'wca_scrambles_info.csv'

# 3. concat.py çš„è¾“å‡ºæ–‡ä»¶å (æœ€ç»ˆç»“æœ)
FINAL_OUTPUT_FILENAME = 'wca_scrambles_info_cross.csv'

# 4. cross.csv ä¸­éœ€è¦è¢«ç§»é™¤çš„åˆ—å (åœ¨å·¦å³æ‹¼æ¥å‰)
COLUMN_TO_DROP_IN_CROSS = 'scrambleId'

# 5. ğŸ†• cross.csv çš„å®Œæ•´è¡¨å¤´åˆ—è¡¨
CROSS_CSV_HEADERS = [
    'scrambleId', 'Y_C', 'Y_BL', 'Y_BR', 'Y_FR', 'Y_FL', 'Y_BL_BR', 'Y_BL_FR', 'Y_BL_FL', 'Y_BR_FR', 'Y_BR_FL', 'Y_FR_FL', 'Y_BL_BR_FR', 'Y_BL_BR_FL', 'Y_BL_FR_FL', 'Y_BR_FR_FL', 
    'W_C', 'W_BL', 'W_BR', 'W_FR', 'W_FL', 'W_BL_BR', 'W_BL_FR', 'W_BL_FL', 'W_BR_FR', 'W_BR_FL', 'W_FR_FL', 'W_BL_BR_FR', 'W_BL_BR_FL', 'W_BL_FR_FL', 'W_BR_FR_FL', 
    'O_C', 'O_BL', 'O_BR', 'O_FR', 'O_FL', 'O_BL_BR', 'O_BL_FR', 'O_BL_FL', 'O_BR_FR', 'O_BR_FL', 'O_FR_FL', 'O_BL_BR_FR', 'O_BL_BR_FL', 'O_BL_FR_FL', 'O_BR_FR_FL', 
    'R_C', 'R_BL', 'R_BR', 'R_FR', 'R_FL', 'R_BL_BR', 'R_BL_FR', 'R_BL_FL', 'R_BR_FR', 'R_BR_FL', 'R_FR_FL', 'R_BL_BR_FR', 'R_BL_BR_FL', 'R_BL_FR_FL', 'R_BR_FR_FL', 
    'G_C', 'G_BL', 'G_BR', 'G_FR', 'G_FL', 'G_BL_BR', 'G_BL_FR', 'G_BL_FL', 'G_BR_FR', 'G_BR_FL', 'G_FR_FL', 'G_BL_BR_FR', 'G_BL_BR_FL', 'G_BL_FR_FL', 'G_BR_FR_FL', 
    'B_C', 'B_BL', 'B_BR', 'B_FR', 'B_FL', 'B_BL_BR', 'B_BL_FR', 'B_BL_FL', 'B_BR_FR', 'B_BR_FL', 'B_FR_FL', 'B_BL_BR_FR', 'B_BL_BR_FL', 'B_BL_FR_FL', 'B_BR_FR_FL'
]

# --- æ ¸å¿ƒå‡½æ•° 1: ä¸Šä¸‹åˆå¹¶æ‰€æœ‰ CSV (åŸ append.py çš„æ ¸å¿ƒé€»è¾‘) ---
def step_1_append_all_csvs(output_filename: str, headers: list):
    """
    æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹æ‰€æœ‰ CSV æ–‡ä»¶ï¼ˆæ’é™¤è¾“å‡ºæ–‡ä»¶å’Œä¸»ä¿¡æ¯æ–‡ä»¶ï¼‰ï¼Œå°†å…¶ä¸Šä¸‹åˆå¹¶ï¼Œ
    å¹¶ä¿å­˜ä¸ºæ–°çš„ CSV æ–‡ä»¶ï¼ŒåŒæ—¶åŠ å…¥æŒ‡å®šçš„è¡¨å¤´ã€‚
    """
    print("="*50)
    print("ğŸš€ æ­¥éª¤ 1/2: å‚ç›´åˆå¹¶ (Append) æ‰€æœ‰ CSV æ–‡ä»¶ï¼Œå¹¶æ·»åŠ è¡¨å¤´")
    print("="*50)

    all_filenames = glob.glob('*.csv')
    
    # æ’é™¤è¾“å‡ºæ–‡ä»¶ã€ä¸»ä¿¡æ¯æ–‡ä»¶å’Œæœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    csv_files_to_merge = [
        f for f in all_filenames 
        if f != output_filename 
        and f != CONCAT_INPUT_DF1
        and f != FINAL_OUTPUT_FILENAME
    ]

    if not csv_files_to_merge:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°å¯ä¾›åˆå¹¶çš„ CSV æ–‡ä»¶ã€‚è·³è¿‡åˆå¹¶æ­¥éª¤ã€‚")
        return None

    all_dataframes = []
    
    for filename in csv_files_to_merge:
        try:
            # è¯»å…¥æ—¶ï¼Œæ–‡ä»¶æœ¬èº«ä»æ˜¯æ— è¡¨å¤´çš„ (header=None)
            df = pd.read_csv(filename, header=None)
            
            # ğŸ†• æ£€æŸ¥åˆ—æ•°æ˜¯å¦åŒ¹é…æ–°è¡¨å¤´
            if df.shape[1] != len(headers):
                print(f"âŒ é”™è¯¯: æ–‡ä»¶ {filename} çš„åˆ—æ•° ({df.shape[1]}) ä¸æŒ‡å®šè¡¨å¤´ ({len(headers)}) ä¸åŒ¹é…ã€‚è·³è¿‡è¯¥æ–‡ä»¶ã€‚")
                continue
            
            # ğŸ†• å°†æ–°çš„è¡¨å¤´èµ‹ç»™å½“å‰ DataFrame
            df.columns = headers
            all_dataframes.append(df)
            
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚è·³è¿‡è¯¥æ–‡ä»¶ã€‚")

    if not all_dataframes:
        print("âš ï¸ è­¦å‘Š: æ‰€æœ‰å¯è¯»æ–‡ä»¶éƒ½ä¸ºç©ºæˆ–è¯»å–å¤±è´¥ã€‚åˆå¹¶ç»“æœä¸ºç©ºã€‚")
        return None

    # ä½¿ç”¨ pd.concat ä¸Šä¸‹ï¼ˆå‚ç›´ï¼‰åˆå¹¶æ‰€æœ‰ DataFrame
    combined_df = pd.concat(all_dataframes, axis=0, ignore_index=True)

    # ğŸ†• ä¿å­˜åˆå¹¶åçš„ç»“æœï¼šheader=True è¡¨ç¤ºå°†åˆ—å (å³æŒ‡å®šçš„ headers) å†™å…¥æ–‡ä»¶
    combined_df.to_csv(output_filename, index=False, header=True)

    print(f"âœ… æˆåŠŸåˆå¹¶ {len(csv_files_to_merge)} ä¸ª CSV æ–‡ä»¶ã€‚")
    print(f"ç»“æœå·²ä¿å­˜åˆ°ä¸­é—´æ–‡ä»¶: **{output_filename}** ({len(combined_df)} è¡Œ, {len(headers)} åˆ—) **å¹¶å·²æ·»åŠ è¡¨å¤´**ã€‚")
    return output_filename

# --- æ ¸å¿ƒå‡½æ•° 2: æ£€æŸ¥è¿ç»­é‡å¤è¡Œ (åŸ CheckConsecutiveDuplicates_ExclFirstCol.py çš„æ ¸å¿ƒé€»è¾‘) ---
def check_consecutive_duplicate_rows(file_path: str, column_to_drop: str) -> bool:
    """
    æ£€æŸ¥ CSV æ–‡ä»¶ä¸­ï¼ˆæ’é™¤æŒ‡å®šçš„åˆ—ï¼Œå³ scrambleIdï¼‰æ˜¯å¦å­˜åœ¨è¿ç»­ç›¸åŒçš„ä¸¤è¡Œï¼Œå¹¶æ‰“å°ç»“æœã€‚
    ç”±äº cross.csv ç°åœ¨æœ‰è¡¨å¤´ï¼Œè¯»å–æ–¹å¼å·²è°ƒæ•´ã€‚
    """
    print("\n"+"-"*50)
    print("ğŸ” å¯é€‰æ£€æŸ¥: æ£€æŸ¥è¿ç»­é‡å¤è¡Œ (æ’é™¤ç¬¬ä¸€åˆ—/scrambleId)")
    print("-" * 50)
    
    if not os.path.exists(file_path):
        print(f"é”™è¯¯: æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ï¼Œæ— æ³•æ‰§è¡Œæ£€æŸ¥ã€‚")
        return False
        
    try:
        # ğŸ†• ç”±äº cross.csv ç°åœ¨æœ‰è¡¨å¤´ï¼Œæˆ‘ä»¬ä½¿ç”¨ header=0 æ¥è¯»å–
        df = pd.read_csv(file_path, header=0)
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

    if df.empty or len(df.columns) <= 1:
        print("è­¦å‘Š: æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–åˆ—æ•°ä¸è¶³ã€‚è·³è¿‡æ£€æŸ¥ã€‚")
        return False

    # ç¡®å®šç”¨äºæ£€æŸ¥çš„ DataFrame (æ’é™¤ 'scrambleId' åˆ—)
    if column_to_drop not in df.columns:
        print(f"âŒ é”™è¯¯: è¦æ’é™¤çš„åˆ— '{column_to_drop}' ä¸åœ¨æ–‡ä»¶è¡¨å¤´ä¸­ã€‚è·³è¿‡æ£€æŸ¥ã€‚")
        return False
        
    df_to_check = df.drop(columns=[column_to_drop])
    
    # æ ¸å¿ƒé€»è¾‘ï¼šæ¯”è¾ƒå½“å‰è¡Œå’Œå‰ä¸€è¡Œ
    consecutive_duplicates = (df_to_check == df_to_check.shift(1)).all(axis=1)

    # è·å–é‡å¤è¡Œçš„ç´¢å¼•
    duplicate_indices = consecutive_duplicates[consecutive_duplicates].index.tolist()
    
    if duplicate_indices:
        pretty_row_numbers = [idx + 2 for idx in duplicate_indices] # +2 å› ä¸º header=0 ä¸”ç´¢å¼•ä» 0 å¼€å§‹
        print(f"ğŸš¨ å‘ç°è¿ç»­é‡å¤è¡Œ (æ’é™¤ {column_to_drop})ã€‚")
        print(f"é‡å¤å¯¹ä¸­ç¬¬äºŒè¡Œçš„è¡Œå· (åŸºäº 1): **{pretty_row_numbers}**")
        print("å»ºè®®æ‰‹åŠ¨æ£€æŸ¥å¹¶æ¸…ç† 'cross.csv' æ–‡ä»¶ã€‚")
        return True
    else:
        print("âœ… æœªå‘ç°è¿ç»­é‡å¤è¡Œ (æ’é™¤ç¬¬ä¸€åˆ—)ã€‚æ•°æ®è´¨é‡è‰¯å¥½ã€‚")
        return False

# --- æ ¸å¿ƒå‡½æ•° 3: å·¦å³æ‹¼æ¥ (åŸ concat.py çš„æ ¸å¿ƒé€»è¾‘) ---
def step_2_concat_files(df1_path: str, df2_path: str, column_to_drop: str, output_filename: str):
    """
    å°†ä¸¤ä¸ª CSV æ–‡ä»¶å·¦å³åˆå¹¶ã€‚df2 (cross.csv) å¿…é¡»å…ˆç§»é™¤æŒ‡å®šçš„é‡å¤åˆ—ã€‚
    """
    print("\n"+"="*50)
    print("ğŸ› ï¸ æ­¥éª¤ 2/2: æ°´å¹³æ‹¼æ¥ (Concat) æ–‡ä»¶")
    print("="*50)

    if not os.path.exists(df1_path):
        print(f"âŒ é”™è¯¯: ä¸»æ–‡ä»¶ **{df1_path}** æœªæ‰¾åˆ°ã€‚æ— æ³•è¿›è¡Œæ‹¼æ¥ã€‚")
        return
        
    if not os.path.exists(df2_path):
        print(f"âŒ é”™è¯¯: ä¸­é—´æ–‡ä»¶ **{df2_path}** æœªæ‰¾åˆ°ã€‚æ— æ³•è¿›è¡Œæ‹¼æ¥ã€‚")
        return

    try:
        # 1. è¯»å–ä¸¤ä¸ª CSV æ–‡ä»¶
        df1 = pd.read_csv(df1_path) # å‡è®¾ wca_scrambles_info.csv æœ‰è¡¨å¤´
        
        # ğŸ†• cross.csv ç°åœ¨æœ‰è¡¨å¤´ï¼Œå¯ä»¥ç›´æ¥è¯»å–
        df2 = pd.read_csv(df2_path)
        
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

    # 2. ç§»é™¤ç¬¬äºŒä¸ªæ–‡ä»¶ä¸­é‡å¤çš„åˆ— (å³ scrambleId)
    if column_to_drop not in df2.columns:
        print(f"âŒ é”™è¯¯: è¦åˆ é™¤çš„åˆ— '{column_to_drop}' ä¸åœ¨ {df2_path} çš„è¡¨å¤´ä¸­ã€‚æ— æ³•ç»§ç»­æ‹¼æ¥ã€‚")
        return

    df2_data = df2.drop(columns=[column_to_drop])

    # 3. å·¦å³åˆå¹¶ï¼šaxis=1 è¡¨ç¤ºæŒ‰åˆ—æ‹¼æ¥ (åŸºäºç´¢å¼•)
    if len(df1) != len(df2_data):
        print(f"âš ï¸ è­¦å‘Š: ä¸¤ä¸ªæ–‡ä»¶çš„è¡Œæ•°ä¸ä¸€è‡´ï¼df1 ({df1_path}) æœ‰ {len(df1)} è¡Œï¼Œdf2 ({df2_path}) æœ‰ {len(df2_data)} è¡Œã€‚")
        print("å°†å¼ºåˆ¶åŸºäºç´¢å¼•æ‹¼æ¥ï¼Œç»“æœå¯èƒ½ä¸å¯é ã€‚")
        
    merged_df = pd.concat([df1, df2_data], axis=1)

    # 4. ä¿å­˜ä¸ºæ–°çš„ CSV æ–‡ä»¶
    merged_df.to_csv(output_filename, index=False)

    print(f"âœ… ä½¿ç”¨ concat æ–¹æ³•åˆå¹¶å®Œæˆï¼")
    print(f"æ–°æ–‡ä»¶ **{output_filename}** å·²ç”Ÿæˆ ({len(merged_df)} è¡Œ, {len(merged_df.columns)} åˆ—)ã€‚")

# --- æ ¸å¿ƒå‡½æ•° 4: åˆ é™¤ä¸­é—´æ–‡ä»¶ ---
def clean_up_intermediate_file(file_path: str):
    """
    åˆ é™¤æŒ‡å®šçš„ä¸­é—´æ–‡ä»¶ã€‚
    """
    if os.path.exists(file_path):
        try:
            os.remove(file_path)
            print(f"\nğŸ—‘ï¸ æˆåŠŸåˆ é™¤ä¸­é—´æ–‡ä»¶: **{file_path}**")
        except Exception as e:
            print(f"\nâŒ åˆ é™¤æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        print(f"\nâ„¹ï¸ ä¸­é—´æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")


# ====================================================================
# ä¸»æ‰§è¡Œå—
# ====================================================================

if __name__ == "__main__":
    
    # 1. æ‰§è¡Œä¸Šä¸‹åˆå¹¶ (append.py)ï¼Œå¹¶æ·»åŠ è¡¨å¤´
    merged_cross_file = step_1_append_all_csvs(APPEND_OUTPUT_FILENAME, CROSS_CSV_HEADERS)
    
    if merged_cross_file:
        
        # 2. æ‰§è¡Œå¯é€‰çš„è¿ç»­é‡å¤è¡Œæ£€æŸ¥ (CheckConsecutiveDuplicates_ExclFirstCol.py)
        check_consecutive_duplicate_rows(APPEND_OUTPUT_FILENAME, COLUMN_TO_DROP_IN_CROSS)
        
        # 3. æ‰§è¡Œå·¦å³æ‹¼æ¥ (concat.py)
        step_2_concat_files(
            df1_path=CONCAT_INPUT_DF1,
            df2_path=APPEND_OUTPUT_FILENAME,
            column_to_drop=COLUMN_TO_DROP_IN_CROSS,
            output_filename=FINAL_OUTPUT_FILENAME
        )

        # 4. ğŸ†• åˆ é™¤ä¸­é—´æ–‡ä»¶ cross.csv
        clean_up_intermediate_file(APPEND_OUTPUT_FILENAME)

    else:
        print("\næµç¨‹ç»ˆæ­¢ï¼šæ­¥éª¤ 1 æœªèƒ½æˆåŠŸç”Ÿæˆåˆå¹¶æ–‡ä»¶ã€‚")
        
        # ğŸ†• å¦‚æœæ­¥éª¤ 1 å¤±è´¥ï¼Œä½†æ–‡ä»¶å¯èƒ½å­˜åœ¨ (ä¾‹å¦‚ï¼Œä¹‹å‰è¿è¡Œé—ç•™)ï¼Œä»å°è¯•æ¸…ç†
        clean_up_intermediate_file(APPEND_OUTPUT_FILENAME)
